{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Serial Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "After installing the package, we first load PLite into the current process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Pkg.clone(\"https://github.com/haoyio/PLite.jl\")  # do this once\n",
    "using PLite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problem definition\n",
    "\n",
    "For this example, we define a simple 1-D gridworld type problem where the goal of the agent is to move and stop at the center of the grid. \n",
    "\n",
    "The idea of this section is to define the problem mathematically without fussing about how we might choose to solve it later (e.g., use a discretized representation). In general, given the wide variety of MDP solvers available, it's important not to restrict our problem representation by what solver we might eventually choose. Rather, we want to choose the best solver after understanding our problem (e.g., find some problem structure that we can exploit)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### MDP initialization\n",
    "\n",
    "We first define some constants and initialize the empty `MDP` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# constants\n",
    "const MinX = 0\n",
    "const MaxX = 100\n",
    "const StepX = 20\n",
    "\n",
    "# mdp definition\n",
    "mdp = MDP()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### State and action spaces definition\n",
    "\n",
    "We define the state and action spaces using a factored representation. If we can't factor the representation, we can simply define the space using a single discrete or continuous variable. \n",
    "\n",
    "For each state and action variable, we either define a continuous or discrete variable. Note that we use the state variables' \"natural representation\" and avoid discretizing `x` even though for value iteration we would eventually have to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# state space\n",
    "statevariable!(mdp, \"x\", MinX, MaxX)  # continuous\n",
    "statevariable!(mdp, \"goal\", [\"no\", \"yes\"])  # discrete\n",
    "\n",
    "# action space\n",
    "actionvariable!(mdp, \"move\", [\"W\", \"E\", \"stop\"])  # discrete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Transition and reward functions definition\n",
    "\n",
    "To define either the transition or reward function, we pass in the `MDP` object, the transition function itself, and an ordered set of the transition function's argument names. \n",
    "\n",
    "In the case of the transition function, we can define it using either the $T(s,a)$ or $T(s,a,s')$ format. Our example here uses the former way. In the case of the reward function, we define it using the $R(s,a)$ format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "transition!(mdp,\n",
    "  [\"x\", \"goal\", \"move\", \"x\", \"goal\"],  # note |xp| is an \"x\" variable\n",
    "                                       # note (s,a,s') order\n",
    "  function mytransition(\n",
    "      x::Float64,\n",
    "      goal::AbstractString,\n",
    "      move::AbstractString,\n",
    "      xp::Float64,\n",
    "      goalp::AbstractString)\n",
    "\n",
    "    function internaltransition(x::Float64, goal::AbstractString, move::AbstractString)\n",
    "      function isgoal(x::Float64)\n",
    "        if abs(x - MaxX / 2) < StepX\n",
    "          return \"yes\"\n",
    "        else\n",
    "          return \"no\"\n",
    "        end\n",
    "      end\n",
    "\n",
    "      if isgoal(x) == \"yes\" && goal == \"yes\"\n",
    "        return [([x, isgoal(x)], 1.0)]\n",
    "      end\n",
    "\n",
    "      if move == \"E\"\n",
    "        if x >= MaxX\n",
    "          return [\n",
    "            ([x, isgoal(x)], 0.9),\n",
    "            ([x - StepX, isgoal(x - StepX)], 0.1)]\n",
    "        elseif x <= MinX\n",
    "          return [\n",
    "            ([x, isgoal(x)], 0.2),\n",
    "            ([x + StepX, isgoal(x + StepX)], 0.8)]\n",
    "        else\n",
    "          return [\n",
    "            ([x, isgoal(x)], 0.1),\n",
    "            ([x - StepX, isgoal(x - StepX)], 0.1),\n",
    "            ([x + StepX, isgoal(x + StepX)], 0.8)]\n",
    "        end\n",
    "      elseif move == \"W\"\n",
    "        if x >= MaxX\n",
    "          return [\n",
    "            ([x, isgoal(x)], 0.1),\n",
    "            ([x - StepX, isgoal(x - StepX)], 0.9)]\n",
    "        elseif x <= MinX\n",
    "          return [\n",
    "          ([x, isgoal(x)], 0.9),\n",
    "          ([x + StepX, isgoal(x + StepX)], 0.1)]\n",
    "        else\n",
    "          return [\n",
    "            ([x, isgoal(x)], 0.1),\n",
    "            ([x - StepX, isgoal(x - StepX)], 0.8),\n",
    "            ([x + StepX, isgoal(x + StepX)], 0.1)]\n",
    "        end\n",
    "      elseif move == \"stop\"\n",
    "        return [([x, isgoal(x)], 1.0)]\n",
    "      end\n",
    "    end\n",
    "\n",
    "    statepprobs = internaltransition(x, goal, move)\n",
    "    for statepprob in statepprobs\n",
    "      if xp == statepprob[1][1] && goalp == statepprob[1][2]\n",
    "        return statepprob[2]\n",
    "      end\n",
    "    end\n",
    "    return 0\n",
    "\n",
    "  end\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "reward!(mdp,\n",
    "  [\"x\", \"goal\", \"move\"],  # note (s,a) order\n",
    "                          # note consistency of variables order with transition\n",
    "  function myreward(x::Float64, goal::AbstractString, move::AbstractString)\n",
    "    if goal == \"yes\" && move == \"stop\"\n",
    "      return 1\n",
    "    else\n",
    "      return 0\n",
    "    end\n",
    "  end\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Solver selection\n",
    "\n",
    "So far, we've only been interested in coding up the mathematical formulation of the problem. From here on, we're only interested in selecting the solver and giving additional information to the solver object such that we can solve the MDP (such as the discretization scheme)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Infinite horizon MDP\n",
    "\n",
    "For an infinite horizon problem with discount $\\gamma$, it can be proven that the value of an optimal policy satisfies the *Bellman equation*\n",
    "$$U^{\\star}(s) = \\max_{a}\\left( R\\left(s,a\\right) + \\gamma\\sum_{s'}T\\left(s'\\mid s, a\\right) U^{\\star}\\left(s'\\right) \\right)\\text{.}$$\n",
    "For the convergence proof, see http://web.stanford.edu/class/ee266/lectures/dpproof.pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To initialize the serial value iteration solver, simply type the following. There are default parameters for the algorithm. Namely, `maxiter`, `tol`, and `discount`. These are keyword arguments with default values. If we don't like them, we can change their values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "solver = SerialValueIteration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In PLite, value iteration requires all variables to be discretized. In the above problem, we need to discretize `x`, so we write the following.\n",
    "\n",
    "Note that the solver uses the `GridInterpolations.jl` package for multilinear interpolation to approximate the values between the discretized state variable values if the $T(s, a)$ type transition is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "const StepX = 20\n",
    "discretize_statevariable!(solver, \"x\", StepX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Finite horizon MDP\n",
    "\n",
    "In value iteration, we compute optimal value function $U_{n}$ associated with a finite horizon of $n$ and no discounting. If $n=0$, then $U_{0}(s)=0$ for all $s$. We can compute $U_{n}$ recursively from this base case\n",
    "$$U_{n}(s) = \\max_{a}\\left( R\\left(s,a\\right) + \\sum_{s'}T\\left(s'\\mid s, a\\right) U_{n-1}\\left(s'\\right) \\right)\\text{.}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Notice that the value iteration solver was built with infinite horizon problems in mind. Itâ€™s easy, however, to modify it to solve finite horizon problems by simply changing the parameters of the solvers. \n",
    "\n",
    "When the iterations terminate, it'll give a warning that the maximum number of iterations have been reached. This message was built to warn the user about convergence issues for infinite horizon problems, so just ignore it since we know what we're doing.\n",
    "\n",
    "For an MDP with a horizon of 40 and no discounting, we can define the solver as follows. (Note that we redefined the discretization scheme since it was lost when we overrode the solver variable.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "solver = SerialValueIteration(maxiter=40, discount=1)\n",
    "discretize_statevariable!(solver, \"x\", StepX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Solution and policy extraction\n",
    "\n",
    "The optimal value function $U^{\\star}$ appears on both sides of the equation. Value iteration approximates $U^{\\star}$ by iteratively updating the estimate of $U^{\\star}$ using the above equation. Once we know $U^{\\star}$, we can extract an optimal policy via\n",
    "$$\\pi\\left(s\\right) \\leftarrow \\underset{a} {\\mathrm{argmax}} \\left( R\\left(s,a\\right) + \\gamma\\sum_{s'}T\\left(s'\\mid s, a\\right) U^{\\star}\\left(s'\\right) \\right)\\text{.}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To generate the solution object, we simply input the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "solution = solve(mdp, solver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Finally, to generate the optimal policy function, we type in the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "policy = getpolicy(mdp, solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can then extract the optimal action at a given state by querying `policy` as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "stateq = (12, \"no\")\n",
    "actionq = policy(stateq...)  # equally valid to type actionq = policy(12, \"no\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The result says that we should move \"east,\" or move right on the grid. This makes sense since we're to the left of the grid center. :D"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 0.4.0",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
