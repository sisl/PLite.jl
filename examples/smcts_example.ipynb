{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Monte-Carlo Tree Search\n",
    "\n",
    "We assume you've read the documentation on Serial Value Iteration. Otherwise, go back [there](http://nbviewer.ipython.org/github/haoyio/PLite.jl/blob/master/examples/svi_example.ipynb) and understand it before coming back."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Monte-Carlo tree search (MCTS) algorithm relies on the same problem definition framework as the value iteration algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Like value iteration, MCTS works by keeping an internal approximation of the value function and chooses the action using it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Unlike value iteration, however, MCTS is an online algorithm. This means that the MCTS policy may start off poor, but it gets better the more it interacts with the MDP simulator/environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The main advantage to MCTS is its ability to give a good approximation of the state-action utility function despite not needing an expensive value iteration-type computation. We recommend using this for problems with large state and/or action spaces.\n",
    "\n",
    "Note however that a key assumption is that both the action space and the state space are finite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Solver definition\n",
    "\n",
    "The syntax for using a serial MCTS solver is similar to that of the serial value iteration solver. We still need to discretize continuous variables since our solver implements the finite MCTS. Otherwise, the only difference is having to initialize a different type of solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "push!(LOAD_PATH, \"../src\")\n",
    "using PLite\n",
    "\n",
    "# constants\n",
    "const MinX = 0\n",
    "const MaxX = 100\n",
    "const StepX = 20\n",
    "\n",
    "# mdp definition\n",
    "mdp = MDP()\n",
    "\n",
    "statevariable!(mdp, \"x\", MinX, MaxX)  # continuous\n",
    "statevariable!(mdp, \"goal\", [\"no\", \"yes\"])  # discrete\n",
    "\n",
    "actionvariable!(mdp, \"move\", [\"W\", \"E\", \"stop\"])  # discrete\n",
    "\n",
    "function isgoal(x::Float64)\n",
    "  if abs(x - MaxX / 2) < StepX\n",
    "    return \"yes\"\n",
    "  else\n",
    "    return \"no\"\n",
    "  end\n",
    "end\n",
    "\n",
    "function mytransition(x::Float64, goal::AbstractString, move::AbstractString)\n",
    "  if isgoal(x) == \"yes\" && goal == \"yes\"\n",
    "    return [([x, isgoal(x)], 1.0)]\n",
    "  end\n",
    "\n",
    "  if move == \"E\"\n",
    "    if x >= MaxX\n",
    "      return [\n",
    "        ([x, isgoal(x)], 0.9),\n",
    "        ([x - StepX, isgoal(x - StepX)], 0.1)]\n",
    "    elseif x <= MinX\n",
    "      return [\n",
    "        ([x, isgoal(x)], 0.2),\n",
    "        ([x + StepX, isgoal(x + StepX)], 0.8)]\n",
    "    else\n",
    "      return [\n",
    "        ([x, isgoal(x)], 0.1),\n",
    "        ([x - StepX, isgoal(x - StepX)], 0.1),\n",
    "        ([x + StepX, isgoal(x + StepX)], 0.8)]\n",
    "    end\n",
    "  elseif move == \"W\"\n",
    "    if x >= MaxX\n",
    "      return [\n",
    "        ([x, isgoal(x)], 0.1),\n",
    "        ([x - StepX, isgoal(x - StepX)], 0.9)]\n",
    "    elseif x <= MinX\n",
    "      return [\n",
    "      ([x, isgoal(x)], 0.9),\n",
    "      ([x + StepX, isgoal(x + StepX)], 0.1)]\n",
    "    else\n",
    "      return [\n",
    "        ([x, isgoal(x)], 0.1),\n",
    "        ([x - StepX, isgoal(x - StepX)], 0.8),\n",
    "        ([x + StepX, isgoal(x + StepX)], 0.1)]\n",
    "    end\n",
    "  elseif move == \"stop\"\n",
    "    return [([x, isgoal(x)], 1.0)]\n",
    "  end\n",
    "end\n",
    "\n",
    "function myreward(x::Float64, goal::AbstractString, move::AbstractString)\n",
    "  if goal == \"yes\" && move == \"stop\"\n",
    "    return 1\n",
    "  else\n",
    "    return 0\n",
    "  end\n",
    "end\n",
    "\n",
    "transition!(mdp, [\"x\", \"goal\", \"move\"], mytransition)\n",
    "reward!(mdp, [\"x\", \"goal\", \"move\"], myreward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We define the solver as follows, and then generate the policy using the same syntax as in the value iteration algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# solver options\n",
    "solver = SerialMCTS()\n",
    "discretize_statevariable!(solver, \"x\", StepX)\n",
    "\n",
    "# generate results\n",
    "solution = solve(mdp, solver)\n",
    "policy = getpolicy(mdp, solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Online solution\n",
    "\n",
    "As mentioned, the policy generally improves as it receives more queries. MCTS grows an internal tree that keeps track of the approximate value function for the states it has seen. For example, after the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "stateq = (20.0, \"no\")\n",
    "actionq = policy(stateq...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We see that the tree has grown, and the resulting state-action value function approximation agrees with intuition (higher value for better actions at a given state)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "actions = [\"W\", \"E\", \"stop\"]\n",
    "for entry in solution.tree\n",
    "    println(\"state: \", entry[1])\n",
    "    println(\"value: \")\n",
    "    for iaction in 1:length(actions)\n",
    "        println(\"\\taction: \", actions[iaction], \", value: \", entry[2].qval[iaction]) \n",
    "    end\n",
    "    println()\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 0.4.0",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
